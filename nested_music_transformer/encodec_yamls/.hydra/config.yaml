nn_params:
  vocab_name: MusicTokenVocabNB
  model_name: DoubleSequentialTransformer
  input_embedder_name: SummationEmbedder
  main_decoder_name: XtransformerDecoder
  sub_decoder_name: CrossAttention_Strategy
  input_embedder:
    num_layer: 1
    num_head: 8
    dropout: 0.1
  main_decoder:
    dim_model: 512
    num_layer: 6
    num_head: 8
    dropout: 0.1
  sub_decoder:
    rnn_hidden_size: 1024
    decout_window_size: 1
    num_layer: 1
    feature_enricher_use: true
  emb:
    emb_size: 512
    total_size: -1
    type: 0.0625
    tempo: 0.25
    chord: 0.5
    beat: 0.125
    pitch: 1
    duration: 0.25
    velocity: 0.25
  prediction_order:
  - pitch
  - duration
  - velocity
  - type
  - beat
  - chord
  - tempo
dataset: Pop1k7
use_ddp: false
use_fp16: true
train_params:
  device: cuda
  batch_size: 8
  grad_clip: 1.0
  num_iter: 100000
  num_cycles_for_inference: 1
  num_cycles_for_model_checkpoint: 5
  iterations_per_training_cycle: 10
  iterations_per_validation_cycle: 3000
  input_length: 1024
  focal_alpha: 1
  focal_gamma: 1
  scheduler: cosinelr
  initial_lr: 0.0001
  decay_step_rate: 0.8
  num_steps_per_cycle: 20000
  warmup_steps: 2000
  max_lr: 0.00015
  gamma: 0.6
  emb_share: false
  world_size: 0
inference_params:
  n_uncond_generation: 3
  n_cond_generation: 1
data_params:
  num_features: 7
  encoding_scheme: nb
  first_pred_feature: pitch
  split_ratio: 0.8
  aug_type: random
general:
  debug: false
  make_log: false
  save_attn_output: true
  infer_and_log: true
  log_train_metric: true
  log_all_feature_loss: true
  save_dir: runs/
