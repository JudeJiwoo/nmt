vocab_name: MusicTokenVocabNB
model_name: DoubleSequentialTransformer
input_embedder_name: SummationEmbedder
main_decoder_name: XtransformerDecoder
sub_decoder_name: CatVec_Strategy
input_embedder:
  num_layer: 1
  num_head: 8
  dropout: 0.1
main_decoder:
  input_length: 1024
  dim_model: 512
  num_layer: 6
  num_head: 8
  dropout: 0.1
sub_decoder:
  rnn_hidden_size: 1024
  decout_window_size: 1 # 1 means no previous decoding output added
  num_layer: 1
emb:
  emb_size: 512
  total_size: -1
  type: 0.0625 # cp 32
  tempo: 0.25 # cp 128
  chord: 0.5 # cp 256
  beat: 0.125 # 64
  pitch: 1 # cp 512
  duration: 0.25 # cp 128
  velocity: 0.25 # cp 128
emb_mlp_layer:
  use: False
  num_layer: 2
  hidden_size: 2048
  dropout: 0.1 
decoding_complexity_layer:
  name: null # "None", "relu", "MLP", "multiMLP", "residualMLP"
  num_layers: 2
  hidden_size: 2048
  dropout: 0.3 

# sequential prediction pitch 1st
prediction_order: ['pitch', 'duration', 'velocity', 'type', 'beat', 'chord', 'tempo']

# sequential prediction type 1st
# prediction_order: ['type', 'beat', 'chord', 'tempo', 'instrument', 'pitch', 'duration', 'velocity']